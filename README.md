ğŸ“„ RAG con Streamlit y Ollama

Este proyecto permite cargar documentos PDF y hacerles preguntas utilizando un enfoque de RAG (Retrieval-Augmented Generation), todo dentro de una interfaz conversacional tipo ChatGPT construida con Streamlit.

ğŸš€ CaracterÃ­sticas

Carga dinÃ¡mica de archivos PDF.

ExtracciÃ³n de texto y segmentaciÃ³n en chunks configurables.

RecuperaciÃ³n semÃ¡ntica con vectores (Chroma + embeddings de Ollama).

GeneraciÃ³n de respuestas con modelos locales como llama3 a travÃ©s de Ollama.

VisualizaciÃ³n de historial conversacional.

VisualizaciÃ³n de metadatos detallados de cada respuesta (tokens, duraciÃ³n, modelo, etc).

ğŸ§± Estructura del proyecto

RAG/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py         # Interfaz Streamlit principal
â”‚   â””â”€â”€ rag.py         # LÃ³gica de RAG: embeddings, recuperaciÃ³n, generaciÃ³n
â”œâ”€â”€ .venv/             # Entorno virtual (opcional)
â”œâ”€â”€ chroma_langchain_db/ # Persistencia del vector store (se crea al correr)
â”œâ”€â”€ requirements.txt   # Dependencias del proyecto
â””â”€â”€ README.md          # Este archivo

âš™ï¸ Requisitos

Python 3.10+

Ollama instalado y corriendo

Modelos como llama3 y nomic-embed-text descargados en Ollama

Instala las dependencias:

pip install -r requirements.txt

Ejecuta Ollama en segundo plano (si no estÃ¡ activo):

ollama run llama3
ollama run nomic-embed-text

â–¶ CÃ³mo ejecutar la aplicaciÃ³n

Desde la raÃ­z del proyecto:

streamlit run src/app.py

Esto abrirÃ¡ automÃ¡ticamente la interfaz en tu navegador (por defecto: http://localhost:8501).

ğŸ§  Â¿CÃ³mo usarlo?

En la barra lateral, carga tu documento PDF.

Ajusta parÃ¡metros como:

Chunk size y overlap

Temperatura, top-k, top-p, max tokens

Escribe tu pregunta en el chat.

La respuesta aparecerÃ¡ junto con sus metadatos dentro de un expander.

Puedes hacer mÃºltiples preguntas sobre el mismo PDF y revisar todo el historial de interacciÃ³n.

ğŸ“¦ Dependencias principales

streamlit

langchain

langchain-community

langchain-chroma

langgraph

ollama

python-dotenv

ğŸ›  PersonalizaciÃ³n

Para cambiar el modelo por defecto, modifica model_name en setup_model_and_vectorstore().

Para cambiar el prompt base, modifica prompt = hub.pull(...) en rag.py.

Puedes almacenar mÃºltiples historiales, exportarlos o integrar autenticaciÃ³n.

ğŸ§ª Notas adicionales

Este proyecto no usa la nube: todo corre localmente.

Requiere una mÃ¡quina con suficiente RAM para usar modelos grandes como llama3.

Puedes extenderlo para mÃºltiples archivos, bÃºsqueda cruzada, o integraciÃ³n con bases de conocimiento.

ğŸ™Œ CrÃ©ditos

Desarrollado como una base prÃ¡ctica para experimentaciÃ³n con Retrieval-Augmented Generation (RAG) local utilizando LangChain, Streamlit y Ollama.