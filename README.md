ğŸ“„ RAG con Streamlit y Ollama

Este proyecto permite cargar documentos PDF y hacerles preguntas utilizando un enfoque de RAG (Retrieval-Augmented Generation), todo dentro de una interfaz conversacional tipo ChatGPT construida con Streamlit.

ğŸš€ CaracterÃ­sticas
- Carga dinÃ¡mica de archivos PDF.
- ExtracciÃ³n de texto y segmentaciÃ³n en chunks configurables.
- RecuperaciÃ³n semÃ¡ntica con vectores (Chroma + embeddings de Ollama).
- GeneraciÃ³n de respuestas con modelos locales a travÃ©s de Ollama.

ğŸ§± Estructura del proyecto
RAG/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py         # Interfaz Streamlit principal
â”‚   â””â”€â”€ rag.py         # LÃ³gica de RAG: embeddings, recuperaciÃ³n, generaciÃ³n
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ documento.pdf  # Documento PDF de prueba
â”œâ”€â”€ .venv/             # Entorno virtual (opcional)
â”œâ”€â”€ chroma_langchain_db/ # Persistencia del vector store (se crea al correr)
â”œâ”€â”€ pyproject.toml     # Dependencias del proyecto
â”œâ”€â”€ gitignore
â””â”€â”€ README.md          # Este archivo

âš™ï¸ Requisitos y dependencias necesarias
Lee las dependencias que estÃ¡n en el documento pyproject.toml
y utilizando uv puedes agregarlas a tu entorno virtual con los comandos "uv add [dependencia]"
con eso ya tienes todas tus dependencias al dÃ­a.

â–¶ CÃ³mo ejecutar la aplicaciÃ³n
Desde la raÃ­z del proyecto:
solo escribe "make stream" y listo tendrÃ¡s tu agente RAG disponible para preguntar sobre tu documento PDF
(Esto abrirÃ¡ automÃ¡ticamente la interfaz en tu navegador).

ğŸ§  Â¿CÃ³mo usarlo?
En la barra lateral, carga tu documento PDF.
- Ajusta parÃ¡metros como:
- Chunk size y overlap
- Temperatura, top-k, top-p, max tokens
- Escribe tu pregunta en el chat.
La respuesta aparecerÃ¡ y ouedes hacer mÃºltiples preguntas sobre el mismo PDF.

ğŸ§ª Notas adicionales
- Este proyecto no usa la nube: todo corre localmente.
- Requiere una mÃ¡quina con suficiente RAM para usar modelos grandes como llama3.
- Puedes extenderlo para mÃºltiples archivos, bÃºsqueda cruzada, o integraciÃ³n con bases de conocimiento.

ğŸ™Œ CrÃ©ditos
Desarrollado como una base prÃ¡ctica para experimentaciÃ³n con Retrieval-Augmented Generation (RAG) local utilizando LangChain, Streamlit y Ollama.